{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dbb01e1-0ff6-4c47-a855-b9ec20266e91",
   "metadata": {},
   "source": [
    "****Project Brief****\n",
    "Problem: Choice Overload. Users waste more time searching than reading because of decision paralysis and a \"winner-takes-all\" market.\n",
    "Solution: A context-aware engine that replaces popularity bias with situational matching (mood, time, environment e.g. bedtime, 10mins, long holiday/travelling).\n",
    "Goal: Reduce decision fatigue and unlock the \"Long Tail\" of publishing/giving niche books the spotlight while helping readers find the perfect book for their current moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c69502-0c20-4bf2-b180-a7bccfdc89ef",
   "metadata": {},
   "source": [
    "****Goodreads Webscraping****\n",
    "Book data required \n",
    "- Genre \n",
    "- Title \n",
    "- Author\n",
    "- Rating\n",
    "- Rating counts \n",
    "- Description \n",
    "- Page numbers \n",
    "- ISBN\n",
    "- Language \n",
    "- Published Year \n",
    "- Book Cover Image \n",
    "- Link to the book 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12d655-e7e7-4d69-a887-ada29e077333",
   "metadata": {},
   "source": [
    "****Open Library API***\n",
    "Identifiers: ISBN-13\n",
    "Physical Specs: Number of pages, physical dimensions, weight, and binding type (Hardcover, mass-market paperback, etc.).\n",
    "\n",
    "Publishing Info: Publisher name, specific publication date, and series name.\n",
    "\n",
    "Table of Contents: Often includes a full list of chapters (a feature many other APIs lack).\n",
    "\n",
    "3. The \"Author\" Layer\n",
    "Open Library treats authors as distinct entities with their own metadata.\n",
    "\n",
    "Biographical Data: Full name, birth/death dates, and a biography.\n",
    "\n",
    "Identifiers: Links to external authority files like VIAF, Wikidata, and Library of Congress ID.\n",
    "\n",
    "Photos: Portraits of the author when available.\n",
    "\n",
    "4. Digital & Community Data\n",
    "Because Open Library is part of the Internet Archive, it includes unique \"living\" data:\n",
    "\n",
    "Availability: Data on whether an eBook version is available to borrow, read online, or download.\n",
    "\n",
    "Community Activity: User-generated Reading Logs (Want to Read, Currently Reading, Have Read), public Book Lists, and user ratings.\n",
    "\n",
    "Revision History: Every single change made to a record is stored, meaning you can access previous \"versions\" of a book's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d572664f-9f18-432c-a399-7c9007ee5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a7ff699-7ca1-49e4-98ca-c052dfd92177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#Checking if the webscraping works \n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}\n",
    "r = requests.get(\"https://www.goodreads.com/\", headers=headers)\n",
    "print (r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebfe366-3121-4612-9916-5bc198f1c184",
   "metadata": {},
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser' ) \n",
    "print (soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2be1c7c-1dfb-424b-9de7-9c0b860d829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for specific gender\n",
    "\n",
    "#genre_art = []\n",
    "\n",
    "#for a in soup.select('a.gr-hyperlink[href=\"/genres/art\"]'):\n",
    "    #text = a.get_text(strip=True)\n",
    "    \n",
    "   \n",
    " #if text:\n",
    "        #genre_art.append(text)\n",
    "\n",
    "#print(genre_art)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bdf168-3e72-4e3c-95e7-ca943d9864a0",
   "metadata": {},
   "source": [
    "#genre_art = []\n",
    "#----------------------------\n",
    "#for a in soup.select('a.gr-hyperlink[href=\"/genres/art\"]'):\n",
    "#text = a.get_text(strip=True)\n",
    "#if text:\n",
    "   #     genre_art.append(text)\n",
    "\n",
    "#print(genre_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db1617c7-e78f-463e-87ed-2015d85cf32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#Best Book Ever List from GoodReads \n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}\n",
    "r = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever\", headers=headers)\n",
    "print (r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca936642-2206-4403-bcde-4df081a40838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prettyfing the Best Book Ever Page \n",
    "soup = BeautifulSoup(r.text, 'html.parser' ) \n",
    "#print (soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6afcb-ba09-44b5-9305-98806b1addaf",
   "metadata": {},
   "source": [
    "# Scrape book links from paginated list pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3959c0d-f43c-4cc9-80aa-ad1ee9a8a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE = \"https://www.goodreads.com\"\n",
    "LIST_URL = \"https://www.goodreads.com/list/show/1.Best_Books_Ever\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f84fa20-3a68-4ebf-873f-73e27404840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url: str) -> BeautifulSoup:\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return BeautifulSoup(r.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "794e2ff4-f83a-4854-b5e9-fbaf18987784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_book_links_from_list(soup: BeautifulSoup):\n",
    "\n",
    "    links = set()\n",
    "    for a in soup.select(\"a.bookTitle[href*='/book/show/'], a[href^='/book/show/']\"):\n",
    "        href = a.get(\"href\")\n",
    "        if href:\n",
    "            clean = href.split(\"?\")[0]  # remove tracking params\n",
    "            links.add(urljoin(BASE, clean))\n",
    "    return links\n",
    "\n",
    "book_urls = []\n",
    "seen = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cbc7cb6-2bf1-405a-9cd9-f45da5ddb02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching list page 1: https://www.goodreads.com/list/show/1.Best_Books_Ever\n"
     ]
    },
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFeatureNotFound\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m url = LIST_URL \u001b[38;5;28;01mif\u001b[39;00m page == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLIST_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m?page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFetching list page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m soup = \u001b[43mget_soup\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m page_links = extract_book_links_from_list(soup)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# add new links\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mget_soup\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m      2\u001b[39m r = requests.get(url, headers=HEADERS, timeout=\u001b[32m30\u001b[39m)\n\u001b[32m      3\u001b[39m r.raise_for_status()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlxml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ironhack/Final_project/.venv/lib/python3.13/site-packages/bs4/__init__.py:366\u001b[39m, in \u001b[36mBeautifulSoup.__init__\u001b[39m\u001b[34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[39m\n\u001b[32m    364\u001b[39m     possible_builder_class = builder_registry.lookup(*features)\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m possible_builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[32m    367\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a tree builder with the features you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    368\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m. Do you need to install a parser library?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m             % \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(features)\n\u001b[32m    370\u001b[39m         )\n\u001b[32m    371\u001b[39m     builder_class = possible_builder_class\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n",
      "\u001b[31mFeatureNotFound\u001b[39m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "page = 1\n",
    "while len(book_urls) < 1000:\n",
    "    url = LIST_URL if page == 1 else f\"{LIST_URL}?page={page}\"\n",
    "    print(f\"Fetching list page {page}: {url}\")\n",
    "\n",
    "    soup = get_soup(url)\n",
    "    page_links = extract_book_links_from_list(soup)\n",
    "\n",
    "    # add new links\n",
    "    new_links = [u for u in page_links if u not in seen]\n",
    "    for u in new_links:\n",
    "        seen.add(u)\n",
    "        book_urls.append(u)\n",
    "        if len(book_urls) >= 1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03b4606e-e71e-45cb-b88d-54f67d6ff590",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'page_links' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m  \u001b[38;5;66;03m# if the page had no book links, stop (means layout changed or blocked)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mpage_links\u001b[49m:\n\u001b[32m      3\u001b[39m  \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo book links found on this page. Stopping.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'page_links' is not defined"
     ]
    }
   ],
   "source": [
    " # if the page had no book links, stop (means layout changed or blocked)\n",
    "if not page_links:\n",
    " print(\"No book links found on this page. Stopping.\")\n",
    "break\n",
    "\n",
    "page += 1\n",
    "time.sleep(1.5)  # be polite\n",
    "\n",
    "print(\"Collected book URLs:\", len(book_urls))\n",
    "print(\"Example:\", book_urls[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb7f7e5-6635-4395-b721-5259617173d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
