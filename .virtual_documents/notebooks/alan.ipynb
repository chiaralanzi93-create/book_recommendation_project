








import pandas as pd
from bs4 import BeautifulSoup
import requests
import time


#Checking if the webscraping works 
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}
r = requests.get("https://www.goodreads.com/", headers=headers)
print (r.status_code)


soup = BeautifulSoup(r.text, 'html.parser' ) 
print (soup.prettify())


#create the dictionary of genre list 
genres_list = {}
for a in soup.select("div a.gr-hyperlink href=genres/art"):


#



genre_art = []

for a in soup.select('a.gr-hyperlink[href="/genres/art"]'):
    text = a.get_text(strip=True)
    
    if text:
        genre_art.append(text)

print(genre_art)





#Best Book Ever List from GoodReads 
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}
r = requests.get("https://www.goodreads.com/list/show/1.Best_Books_Ever", headers=headers)
print (r.status_code)


#Prettyfing the Best Book Ever Page 
soup = BeautifulSoup(r.text, 'html.parser' ) 
print (soup.prettify())


#Web Scraping of Best Book Ever List 
# Base URL for the list
base_url = "https://www.goodreads.com/list/show/1.Best_Books_Ever"
page_to_scrape = 1  # Starting page

def scrape_book_details(book_url):
    # This simulates "clicking" into the book
    full_url = "https://www.goodreads.com" + book_url
    response = requests.get(full_url)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    #Title 
    description = soup.find('div', {'class': 'FullExpandContent'})
    return description.text.strip() if description else "No description"

# Pagination Loop
while page_to_scrape <= 2:  # Let's just do 2 pages for this example
    print(f"--- Scraping Page {page_to_scrape} ---")
    params = {'page': page_to_scrape}
    response = requests.get(base_url, params=params)
    soup = BeautifulSoup(response.text, 'html.parser')

    # 1. Find all book links on the list page
    book_links = soup.find_all('a', class_='bookTitle')

    for link in book_links:
        relative_url = link['href']
        title = link.find('span').text
        print(f"Clicking into: {title}")
        
        # 2. Go inside the book page
        desc = scrape_book_details(relative_url)
        print(f"Description found: {desc[:50]}...")
        
        # Respectful delay so you don't get banned
        time.sleep(1)

    page_to_scrape += 1


#Individual Page 
https://www.goodreads.com/book/show/2767052-the-hunger-games


import requests
from bs4 import BeautifulSoup
import json
import re

def scrape_goodreads_dynamic(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        return f"Error: Status code {response.status_code}"

    soup = BeautifulSoup(response.content, 'html.parser')

    # 1. Dynamic Link Extraction
    book_link = soup.find("link", rel="canonical")["href"] if soup.find("link", rel="canonical") else url

    # 2. Dynamic Image Extraction
    cover_image = ""
    og_image = soup.find("meta", property="og:image")
    if og_image:
        cover_image = og_image["content"]

    # 3. Dynamic Published Year Extraction
    published_year = "Not found"
    
    # Method A: Search for "First published" text node 
    pub_info_node = soup.find(string=re.compile(r"First published", re.IGNORECASE))
    
    if pub_info_node:
        # Extracts any 4-digit sequence (the year) from that specific text string
        year_match = re.search(r'\b\d{4}\b', pub_info_node)
        if year_match:
            published_year = year_match.group(0)
    
    # Method B: Fallback to JSON-LD structured data if the text isn't found
    if published_year == "Not found":
        json_data = soup.find("script", type="application/ld+json")
        if json_data:
            try:
                data = json.loads(json_data.string)
                # Some pages use 'datePublished'
                if "datePublished" in data:
                    published_year = data["datePublished"][:4]
            except:
                pass

    return {
        "Published Year": published_year,
        "Cover Image": cover_image,
        "Book Link": book_link
    }

url = "https://www.goodreads.com/book/show/2767052-the-hunger-games"
print(json.dumps(scrape_goodreads_dynamic(url), indent=4))


books = pd.DataFrame ({})


#[API] 
#Reading the JikanAPI to see if the call is successful 
jikan_url = "http://discord.jikan.moe"
response = requests.get (jikan_url)
response.status_code


#[API]
#Testing the API call with 3 Manga originated Global Intellectual Property Hits; 'One Piece', 'Kimetsu no Yaiba (Demon Slayers)', 'Dandadan'
def get_manga_info(manga_id):
    url = f"https://api.jikan.moe/v4/manga/{manga_id}/full"
    
    try:
        response = requests.get(url)
        
        if response.status_code != 200:
            print(f"Server Error for ID {manga_id}: {response.status_code}")
            return None
            
        if not response.text.strip():
            print(f"Empty envelope for ID {manga_id}. The server sent nothing.")
            return None
            
        return response.json()['data']
        
    except Exception as e:
        print(f"Connection error for ID {manga_id}: {e}")
        return None

ids = [13, 96792, 135496] # One Piece, Demon Slayer, Dandadan
manga_results = []

for m_id in ids:
    data = get_manga_info(m_id)
    if data:
        manga_results.append(data)
        print(f"Successfully pulled: {data['title']}")
    
    # wait 2 seconds before running it again 
    time.sleep(2)



